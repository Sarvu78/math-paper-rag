{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d951179",
   "metadata": {},
   "source": [
    "# Math-RAG: Interactive Research Paper Analysis\n",
    "Navigating mathematics research papers often leads to a \"citation rabbit hole.\" When a proof relies on a specific Lemma or Theorem from a cited work, researchers must manually find, download, and search through external papers‚Äîa process that breaks cognitive flow.\n",
    "\n",
    "Math-RAG is an intelligent Retrieval-Augmented Generation solution designed to make the citation network interactive. By indexing both the primary paper and its references, it provides immediate access to the technical details you need, exactly when you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Standard Library Imports\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "# ==========================================\n",
    "# 2. Environment & Network Imports\n",
    "# ==========================================\n",
    "import requests\n",
    "from httpx import get\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================\n",
    "# 3. Document Processing (PDFs & Parsing)\n",
    "# ==========================================\n",
    "import fitz  # PyMuPDF for basic PDF manipulation\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# ==========================================\n",
    "# 4. AI & LangChain Framework\n",
    "# ==========================================\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Vector Store & Embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# ==========================================\n",
    "# 5. Specialized Research & Academic Tools\n",
    "# ==========================================\n",
    "import arxiv\n",
    "from rapidfuzz import fuzz\n",
    "from unpywall.utils import UnpywallCredentials\n",
    "from unpywall import Unpywall\n",
    "\n",
    "# ==========================================\n",
    "# 6. Configuration & Initialization\n",
    "# ==========================================\n",
    "\n",
    "# Load environment variables (API Keys, etc.)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Unpywall for open-access paper retrieval\n",
    "UnpywallCredentials('sarvagya07jain@gmail.com')\n",
    "\n",
    "# Constants\n",
    "CROSSREF_API = \"https://api.crossref.org\"\n",
    "\n",
    "# Directory Setup\n",
    "# Note: Using Pathlib is generally safer for cross-platform paths\n",
    "PROJECT_DIR = Path(\"/Users/sarvagyajain/Downloads/Programming/math-papers-rag\")\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Opens a PDF and locates the start of the bibliography section.\n",
    "    Returns all text following the 'References' or 'Bibliography' header.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    # Extract text from all pages and join into a single string\n",
    "    text = \"\".join(p.get_text() for p in doc)\n",
    "\n",
    "    # Search for the headers commonly used in academic papers\n",
    "    match = re.search(\n",
    "        r\"\\nreferences\\n|\\nbibliography\\n\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    if not match:\n",
    "        raise ValueError(\"References section not found - check if the PDF uses a different header.\")\n",
    "\n",
    "    # Return only the text starting from the end of the matched header\n",
    "    return text[match.end():]\n",
    "\n",
    "def split_references(ref_text):\n",
    "    \"\"\"\n",
    "    Attempts to split a block of reference text into individual citations \n",
    "    using common academic numbering and labeling patterns.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"\\n\\s*\\[\\d+\\]\\s*\",       # Matches [1], [2], etc.\n",
    "        r\"\\n\\s*\\d+\\.\\s*\",         # Matches 1., 2., etc.\n",
    "        r\"\\n\\s*\\[[A-Z]{2,}\\d{2}\\]\\s*\" # Matches alphanumeric labels like [SJ24]\n",
    "    ]\n",
    "\n",
    "    for p in patterns:\n",
    "        parts = re.split(p, ref_text)\n",
    "        # Heuristic: If we found more than 3 parts, the pattern likely worked.\n",
    "        # We also filter for length (>50 chars) to ignore noise or page numbers.\n",
    "        if len(parts) > 3:\n",
    "            return [r.strip() for r in parts if len(r.strip()) > 50]\n",
    "\n",
    "    # Fallback: If no pattern fits, split by newline and filter for longer strings.\n",
    "    return [r.strip() for r in ref_text.split(\"\\n\") if len(r.strip()) > 80]\n",
    "\n",
    "def normalize_reference_text(ref):\n",
    "    \"\"\"\n",
    "    Cleans up a raw reference string by removing excessive whitespace, \n",
    "    new lines, and potential page artifacts.\n",
    "    \"\"\"\n",
    "    # Replace all whitespace (tabs, multiple spaces, newlines) with a single space\n",
    "    ref = re.sub(r\"\\s+\", \" \", ref)\n",
    "    ref = ref.replace(\"\\n\", \" \")\n",
    "    ref = ref.strip(\" .;,\")\n",
    "\n",
    "    # Filter out potential artifacts (e.g., page headers or footers) \n",
    "    # that are too short to be a valid citation.\n",
    "    if len(ref.split()) < 6:\n",
    "        return None\n",
    "\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d52bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_title_extraction(reference):\n",
    "    if not reference:\n",
    "        return None\n",
    "\n",
    "    # 1. PRE-PROCESSING: Clean up the raw string\n",
    "    # Replace newlines with spaces and handle hyphenated line breaks (mag-nitude -> magnitude)\n",
    "    ref = reference.replace('\\n', ' ')\n",
    "    ref = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', ref) \n",
    "    \n",
    "    # Remove leading citation numbers like [12] or 12.\n",
    "    ref = re.sub(r'^\\[?\\d+\\]?\\s*', '', ref)\n",
    "\n",
    "    # 2. STRATEGY: Split into candidates\n",
    "    # We split by common delimiters: Periods, Commas, and Semicolons\n",
    "    candidates = re.split(r'[.,;]\\s+', ref)\n",
    "    \n",
    "    best_candidate = None\n",
    "    highest_score = -100\n",
    "\n",
    "    # Markers that suggest a segment is NOT a title\n",
    "    forbidden_markers = {\n",
    "        'proc', 'vol', 'no', 'pp', 'pages', 'journal', 'theory', 'press', \n",
    "        'university', 'arxiv', 'preprint', 'appear', 'edited', 'eds', 'acta', \n",
    "        'annals', 'math', 'soc', 'inst', 'conf', 'berkeley', 'cambridge', 'ann'\n",
    "    }\n",
    "    \n",
    "    # Math markers that increase confidence\n",
    "    math_markers = {\n",
    "        'integers', 'prime', 'factors', 'short', 'intervals', 'smooth', \n",
    "        'analytic', 'riemann', 'hypothesis', 'elliptic', 'curves', 'asymptotic',\n",
    "        'expansion', 'distribution', 'function'\n",
    "    }\n",
    "\n",
    "    for segment in candidates:\n",
    "        segment = segment.strip().strip(\".,()[]\")\n",
    "        words = segment.split()\n",
    "        word_count = len(words)\n",
    "        lower_seg = segment.lower()\n",
    "\n",
    "        if word_count < 3:\n",
    "            continue\n",
    "            \n",
    "        score = 0\n",
    "\n",
    "        # --- Scoring Logic ---\n",
    "        \n",
    "        # Length: Math titles in your list are usually 5-15 words\n",
    "        if 5 <= word_count <= 18:\n",
    "            score += 25\n",
    "        elif 3 <= word_count < 5:\n",
    "            score += 5\n",
    "\n",
    "        # Content: Reward math-heavy terminology\n",
    "        matches = sum(1 for m in math_markers if m in lower_seg)\n",
    "        score += (matches * 8)\n",
    "\n",
    "        # Content: Penalize metadata words (Journal names, \"to appear\", etc.)\n",
    "        # If the segment IS the journal (e.g., \"Int. J. Number Theory\"), penalize heavily\n",
    "        if any(marker in lower_seg for marker in forbidden_markers):\n",
    "            score -= 40\n",
    "            \n",
    "        # Penalize segments that look like author lists (e.g., \"A. Balog\" or \"H. W. Lenstra, Jr.\")\n",
    "        if re.search(r'\\b[A-Z]\\.\\s', segment) or lower_seg.endswith(' jr'):\n",
    "            score -= 30\n",
    "\n",
    "        # Heuristic: Titles often start with \"On \", \"A \", \"The \", \"An \"\n",
    "        if re.match(r'^(on|a|the|an)\\s', lower_seg):\n",
    "            score += 15\n",
    "\n",
    "        if score > highest_score:\n",
    "            highest_score = score\n",
    "            best_candidate = segment\n",
    "\n",
    "    # Final polish: strip trailing punctuation often left by the split\n",
    "    return best_candidate.strip() if best_candidate and highest_score > 0 else None\n",
    "\n",
    "def title_cleanup_manual(raw_title):\n",
    "    \"\"\"\n",
    "    Sanitizes a title string by removing XML/MathML tags, \n",
    "    illegal file characters, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not raw_title:\n",
    "        return None\n",
    "\n",
    "    # 1. Remove MathML tags (e.g., <mml:math>...) frequently found in \n",
    "    # metadata from Crossref or PubMed.\n",
    "    clean_title = re.sub(r'<mml:math[^>]*>.*?<\\/mml:math>', '', raw_title, flags=re.DOTALL)\n",
    "    \n",
    "    # 2. Standardize spacing: Replace newlines with a single space\n",
    "    clean_title = clean_title.replace('\\n', ' ')\n",
    "    \n",
    "    # 3. File System Safety: Remove characters that are illegal in filenames \n",
    "    # (useful if you plan to save the PDF using this title).\n",
    "    clean_title = re.sub(r'[\\\\/:*?\"<>|]', '', clean_title)\n",
    "    \n",
    "    # 4. Final Polish: Collapse multiple spaces into one and lowercase\n",
    "    clean_title = re.sub(r'\\s+', ' ', clean_title).strip()\n",
    "    \n",
    "    return clean_title.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55634411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional block of code for title cleanup using llm.\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-3-pro-preview\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=None,\n",
    ")\n",
    "class CleanReferences(BaseModel):\n",
    "    references: List[str]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are cleaning mathematical bibliography entries.\\n\"\n",
    "     \"You MUST NOT invent references.\\n\"\n",
    "     \"You MUST preserve all factual content.\\n\"\n",
    "     \"You may only normalize formatting, punctuation, and spacing.\\n\"\n",
    "     \"If an entry is not a real bibliographic reference, REMOVE it.\\n\"\n",
    "     \"Return references in MathSciNet-like style.\"\n",
    "    ),\n",
    "    (\"user\",\n",
    "     \"Here are raw references extracted from a math paper:\\n\\n\"\n",
    "     \"{refs}\\n\\n\"\n",
    "     \"Return a JSON object with key 'references'.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "def llm_clean_references_langchain(raw_refs, llm):\n",
    "    parser = JsonOutputParser(pydantic_object=CleanReferences)\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    result = chain.invoke({\n",
    "        \"refs\": \"\\n\".join(f\"- {r}\" for r in raw_refs)\n",
    "    })\n",
    "\n",
    "    return result[\"references\"]\n",
    "\n",
    "\n",
    "class ReferenceMetadata(BaseModel):\n",
    "    title: Optional[str]\n",
    "    authors: Optional[List[str]]\n",
    "\n",
    "metadata_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You extract bibliographic metadata from mathematical references.\\n\"\n",
    "     \"You MUST NOT invent missing information.\\n\"\n",
    "     \"If information is unclear, return null.\\n\"\n",
    "     \"Do NOT guess titles or authors.\\n\"\n",
    "     \"Return structured JSON only.\"\n",
    "    ),\n",
    "    (\"user\",\n",
    "     \"Reference:\\n{ref}\\n\\n\"\n",
    "     \"Extract metadata using the provided schema.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "def llm_extract_reference_metadata(ref, llm):\n",
    "    parser = JsonOutputParser(pydantic_object=ReferenceMetadata)\n",
    "    chain = metadata_prompt | llm | parser\n",
    "\n",
    "    return chain.invoke({\"ref\": ref})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv_pdf(title, path=\"./pdfs\"):\n",
    "    \"\"\"\n",
    "    Searches arXiv for a paper by title, validates the match using fuzzy string \n",
    "    comparison, and downloads the PDF if it meets a 90% similarity threshold.\n",
    "    \"\"\"\n",
    "    # 1. Setup download directory\n",
    "    full_path = PROJECT_DIR / path\n",
    "    full_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 2. Initialize arXiv client and search\n",
    "    client = arxiv.Client()\n",
    "    # We search for the top 5 results to account for similar titles\n",
    "    search = arxiv.Search(query=f\"{title}\", max_results=5)\n",
    "\n",
    "    best_score = 0\n",
    "    best_pdf = None\n",
    "\n",
    "    # 3. Identify the most relevant result\n",
    "    for result in client.results(search):\n",
    "        # fuzzy logic handles slight punctuation or casing differences\n",
    "        score = fuzz.ratio(title.lower(), result.title.lower())\n",
    "        \n",
    "        # We only consider high-confidence matches (>90%)\n",
    "        if score > best_score and score > 90:\n",
    "            best_score = score\n",
    "            best_pdf = result\n",
    "            \n",
    "    # 4. Download if a confident match was found\n",
    "    if best_pdf:\n",
    "        # Generate a clean filename using your title_cleanup helper\n",
    "        safe_filename = f\"{title_cleanup_manual(best_pdf.title)}.pdf\"\n",
    "        download_dest = full_path / safe_filename\n",
    "        \n",
    "        # Use the arxiv library's built-in download method\n",
    "        best_pdf.download_pdf(dirpath=str(full_path), filename=safe_filename)\n",
    "        print(f\"Successfully downloaded: {safe_filename} (Score: {best_score})\")\n",
    "    else:\n",
    "        print(f\"No high-confidence match found for: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eac7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, filename, path=\"./pdfs\"):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from a direct URL (useful for Crossref/Unpywall links).\n",
    "    \"\"\"\n",
    "    # 1. Resolve target directory\n",
    "    full_path = PROJECT_DIR / path\n",
    "    full_path.mkdir(parents=True, exist_ok=True)\n",
    "    target_file = full_path / filename\n",
    "\n",
    "    try:\n",
    "        # 2. Fetch the content with a 20-second timeout\n",
    "        r = requests.get(url, timeout=20)\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            print(f\"Failed to download. Status code: {r.status_code}\")\n",
    "            return None\n",
    "\n",
    "        # 3. Write binary content to file\n",
    "        with open(target_file, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "        return str(target_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during download: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doi_by_title(title, rows=3):\n",
    "    \"\"\"\n",
    "    Queries the Crossref API to find the most likely DOI for a given paper title.\n",
    "    \n",
    "    Returns the DOI of the top result based on Crossref's internal \n",
    "    relevance ranking.\n",
    "    \"\"\"\n",
    "    # Crossref API parameters: \n",
    "    # query.title searches the title field specifically\n",
    "    # rows limits the result set (we usually only need the top match)\n",
    "    params = {\n",
    "        \"query.title\": title,\n",
    "        \"rows\": rows\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(f\"{CROSSREF_API}/works\", params=params, timeout=10)\n",
    "        r.raise_for_status() # Raise an error for 4xx or 5xx responses\n",
    "\n",
    "        # Navigate the JSON response structure\n",
    "        items = r.json()[\"message\"][\"items\"]\n",
    "\n",
    "        if not items:\n",
    "            return None\n",
    "\n",
    "        # Crossref sorts by relevance; we assume the first item is the best match\n",
    "        return items[0].get(\"DOI\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding DOI for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_title_from_doi(doi):\n",
    "    \"\"\"\n",
    "    Performs a reverse lookup: takes a DOI and retrieves the official \n",
    "    title of the paper from Crossref.\n",
    "    \"\"\"\n",
    "    # Direct endpoint for a specific work\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        data = r.json()[\"message\"]\n",
    "        \n",
    "        # 'title' is returned as a list of strings in Crossref metadata\n",
    "        titles = data.get(\"title\", [])\n",
    "\n",
    "        # Return the primary title if it exists\n",
    "        return titles[0] if titles else None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving title for DOI {doi}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71730a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cited_dois(doi):\n",
    "    \"\"\"\n",
    "    Given a primary paper's DOI, fetches its reference list and attempts \n",
    "    to download the full-text PDF for every cited work found.\n",
    "    \"\"\"\n",
    "    # 1. Fetch metadata for the parent paper from Crossref\n",
    "    r = requests.get(\n",
    "        f\"{CROSSREF_API}/works/{doi}\",\n",
    "        timeout=10\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "\n",
    "    message = r.json()[\"message\"]\n",
    "    # Extract the list of cited references\n",
    "    references = message.get(\"reference\", [])\n",
    "\n",
    "    for ref in references:\n",
    "        ref_title = ref.get(\"volume-title\")\n",
    "        ref_doi = ref.get(\"DOI\")\n",
    "\n",
    "        # CASE A: The citation has a DOI (Most reliable)\n",
    "        if ref_doi:\n",
    "            # Try to find a legal Open Access PDF link via Unpywall\n",
    "            url = Unpywall.get_pdf_link(doi=ref_doi)\n",
    "            \n",
    "            # Fetch the official title to use as a filename\n",
    "            raw_title = get_title_from_doi(ref_doi)\n",
    "\n",
    "            if raw_title:\n",
    "                title = title_cleanup_manual(raw_title)\n",
    "            else:\n",
    "                title = \"untitled\" # Fallback for metadata-poor records\n",
    "\n",
    "            if url:\n",
    "                filename = f\"{title}.pdf\"\n",
    "                \n",
    "                # Sanity check: if cleaning the title resulted in an empty string\n",
    "                if not title or title == \"untitled\":\n",
    "                    # Use a sanitized DOI as the filename to prevent overwrites\n",
    "                    filename = f\"{ref_doi.replace('/', '_')}.pdf\"\n",
    "                \n",
    "                print(f\"Downloading cited paper via Unpywall: {filename}\")\n",
    "                download_pdf(url, filename)\n",
    "        \n",
    "        # CASE B: No DOI found, but we have a title (Fallback to arXiv)\n",
    "        elif ref_title:\n",
    "            print(f\"No DOI for citation. Searching arXiv for: {ref_title}\")\n",
    "            search_arxiv_pdf(ref_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_reference_download(title=None, pdf_path=None):\n",
    "    \"\"\"\n",
    "    The main pipeline for gathering research materials. \n",
    "    Can either:\n",
    "    1. Parse a LOCAL PDF to find and download its references.\n",
    "    2. Use a PAPER TITLE to find its DOI, download itself, and crawl its citations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # PATH 1: Processing a Local PDF File\n",
    "    if pdf_path:\n",
    "        # Extract the bibliography section text\n",
    "        ref_text = extract_references_from_pdf(pdf_path)\n",
    "        # Identify individual citations\n",
    "        raw_references = split_references(ref_text)\n",
    "        # Clean and filter out noise/short strings\n",
    "        normalized_references = [normalize_reference_text(r) for r in raw_references]\n",
    "        final_references = [r for r in normalized_references if r]\n",
    "        \n",
    "        # Pull the title from each citation string\n",
    "        titles = [manual_title_extraction(r) for r in final_references if manual_title_extraction(r)]\n",
    "        \n",
    "        for raw_title in titles:\n",
    "            try:\n",
    "                # Direct search on arXiv for these titles\n",
    "                search_arxiv_pdf(raw_title)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading reference from arxiv: {raw_title}, Error: {e}\")\n",
    "\n",
    "    # PATH 2: Discovery via Paper Title\n",
    "    if title:\n",
    "        # Step 2a: Locate the unique identifier (DOI) for the paper\n",
    "        try:\n",
    "            ref_doi = find_doi_by_title(title)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding DOI for title: {title}, Error: {e}\")\n",
    "            return # Exit if we can't identify the primary paper\n",
    "\n",
    "        # Step 2b: Download the primary paper itself\n",
    "        try:\n",
    "            url = Unpywall.get_pdf_link(doi=ref_doi)\n",
    "            raw_title = get_title_from_doi(ref_doi)\n",
    "\n",
    "            # Cleanup name for file system compatibility\n",
    "            if raw_title:\n",
    "                title = title_cleanup_manual(raw_title)\n",
    "            else:\n",
    "                title = \"untitled\"\n",
    "            \n",
    "            if url:\n",
    "                filename = f\"{title}.pdf\"\n",
    "                if not title:\n",
    "                    filename = f\"{ref_doi.replace('/', '_')}.pdf\"\n",
    "                download_pdf(url, filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading main paper PDF: {title}, DOI: {ref_doi}, Error: {e}\")\n",
    "        try:\n",
    "            # If we couldn't download via Unpywall, try arXiv as a fallback\n",
    "            if not url:\n",
    "                print(f\"No Unpywall link found for DOI: {ref_doi}. Attempting arXiv search.\")\n",
    "                search_arxiv_pdf(title)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading main paper PDF from arXiv: {title}, Error: {e}\")\n",
    "\n",
    "        # Step 2c: Recursively download all citations listed in the metadata\n",
    "        try:\n",
    "            download_cited_dois(ref_doi)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading all cited DOIs for DOI: {ref_doi}, Error: {e}\")\n",
    "    \n",
    "    print(\"‚úÖ Completed full reference download pipeline\")\n",
    "\n",
    "    # Safety check for missing arguments\n",
    "    if not title and not pdf_path:\n",
    "        raise ValueError(\"Must provide either a 'title' or a 'pdf_path' to initiate the pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68455c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reference_download(title = \"smooth numbers in short intervals\", pdf_path=\"./pdfs/smooth numbers in short intervals.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619815f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your folder path\n",
    "def partition_documents(input_dir: str):\n",
    "    input_dir = Path(input_dir)\n",
    "    all_elements = []\n",
    "\n",
    "    # 2. Loop through every PDF in the directory\n",
    "    for pdf_file in input_dir.glob(\"*.pdf\"):\n",
    "        print(f\"Processing: {pdf_file.name}...\")\n",
    "        \n",
    "        # 3. Partition the PDF into elements (Titles, NarrativeText, Tables, etc.)\n",
    "        elements = partition_pdf(\n",
    "            filename=str(pdf_file),\n",
    "            # Strategy 'hi_res' is best for RAG as it identifies tables/images\n",
    "            strategy=\"hi_res\", \n",
    "            # Optional: merge small chunks to maintain context\n",
    "            combine_text_under_n_chars=500,\n",
    "            # Extract images or tables if needed for multimodal RAG\n",
    "            extract_images_in_pdf=False \n",
    "        )\n",
    "        \n",
    "        all_elements.extend(elements)\n",
    "\n",
    "\n",
    "    print(f\"Total elements captured: {len(all_elements)}\")\n",
    "    return all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_by_title(elements):\n",
    "    \"\"\"Create intelligent chunks using title-based strategy\"\"\"\n",
    "    print(\"üî® Creating smart chunks...\")\n",
    "    \n",
    "    chunks = chunk_by_title(\n",
    "        elements, # The parsed PDF elements from previous step\n",
    "        max_characters=3000, # Hard limit - never exceed 3000 characters per chunk\n",
    "        new_after_n_chars=2400, # Try to start a new chunk after 2400 characters\n",
    "        combine_text_under_n_chars=500 # Merge tiny chunks under 500 chars with neighbors\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c682584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_content_types(chunk):\n",
    "    \"\"\"Analyze what types of content are in a chunk\"\"\"\n",
    "    content_data = {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'types': ['text']\n",
    "    }\n",
    "    \n",
    "    # Check for tables and images in original elements\n",
    "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            element_type = type(element).__name__\n",
    "            \n",
    "            # Handle tables\n",
    "            if element_type == 'Table':\n",
    "                content_data['types'].append('table')\n",
    "                table_html = getattr(element.metadata, 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "            \n",
    "            # Handle images\n",
    "            elif element_type == 'Image':\n",
    "                if hasattr(element, 'metadata') and hasattr(element.metadata, 'image_base64'):\n",
    "                    content_data['types'].append('image')\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "    \n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d499be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ai_enhanced_summary(text: str, tables: List[str], images: List[str]) -> str:\n",
    "    \"\"\"Create AI-enhanced summary for mixed content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-2.5-flash\",\n",
    "                    temperature=0.0,  # Gemini 3.0+ defaults to 1.0\n",
    "                    max_tokens=None,\n",
    "                    timeout=120000,\n",
    "                    max_retries=10\n",
    "                )\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "\n",
    "        CONTENT TO ANALYZE:\n",
    "        TEXT CONTENT:\n",
    "        {text}\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add tables if present\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i, table in enumerate(tables):\n",
    "                prompt_text += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "        \n",
    "                prompt_text += \"\"\"\n",
    "                YOUR TASK:\n",
    "                Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "                1. Key facts, numbers, and data points from text and tables\n",
    "                2. Main topics and concepts discussed  \n",
    "                3. Questions this content could answer\n",
    "                4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "                5. Alternative search terms users might use\n",
    "\n",
    "                Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "                SEARCHABLE DESCRIPTION:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add images to the message\n",
    "        for image_base64 in images:\n",
    "            message_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "            })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå AI summary failed: {e}\")\n",
    "        # Fallback to simple summary\n",
    "        summary = f\"{text[:300]}...\"\n",
    "        if tables:\n",
    "            summary += f\" [Contains {len(tables)} table(s)]\"\n",
    "        if images:\n",
    "            summary += f\" [Contains {len(images)} image(s)]\"\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_chunks(chunks):\n",
    "    \"\"\"Process all chunks with AI Summaries\"\"\"\n",
    "    print(\"üß† Processing chunks with AI Summaries...\")\n",
    "    \n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk = i + 1\n",
    "        print(f\"   Processing chunk {current_chunk}/{total_chunks}\")\n",
    "        \n",
    "        # Analyze chunk content\n",
    "        content_data = separate_content_types(chunk)\n",
    "        \n",
    "        # Debug prints\n",
    "        print(f\"     Types found: {content_data['types']}\")\n",
    "        print(f\"     Tables: {len(content_data['tables'])}, Images: {len(content_data['images'])}\")\n",
    "        \n",
    "        # Create AI-enhanced summary if chunk has tables/images\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(f\"     ‚Üí Creating AI summary for mixed content...\")\n",
    "            try:\n",
    "                enhanced_content = create_ai_enhanced_summary(\n",
    "                    content_data['text'],\n",
    "                    content_data['tables'], \n",
    "                    content_data['images']\n",
    "                )\n",
    "                print(f\"     ‚Üí AI summary created successfully\")\n",
    "                print(f\"     ‚Üí Enhanced content preview: {enhanced_content[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå AI summary failed: {e}\")\n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(f\"     ‚Üí Using raw text (no tables/images)\")\n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        # Create LangChain Document with rich metadata\n",
    "        doc = Document(\n",
    "            page_content=enhanced_content,\n",
    "            metadata={\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": content_data['text'],\n",
    "                    \"tables_html\": content_data['tables'],\n",
    "                    \"images_base64\": content_data['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        langchain_documents.append(doc)\n",
    "    \n",
    "    print(f\"‚úÖ Processed {len(langchain_documents)} chunks\")\n",
    "    return langchain_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_chunks_to_json(chunks, filename=\"chunks_export.json\"):\n",
    "    \"\"\"Export processed chunks to clean JSON format\"\"\"\n",
    "    export_data = []\n",
    "    \n",
    "    for i, doc in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"enhanced_content\": doc.page_content,\n",
    "            \"metadata\": {\n",
    "                \"original_content\": json.loads(doc.metadata.get(\"original_content\", \"{}\"))\n",
    "            }\n",
    "        }\n",
    "        export_data.append(chunk_data)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Exported {len(export_data)} chunks to {filename}\")\n",
    "    return export_data\n",
    "\n",
    "# Export your chunks\n",
    "# json_data = export_chunks_to_json(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents, persist_directory=\"dbv1/chroma_db\"):\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    \"\"\"Create and persist ChromaDB vector store\"\"\"\n",
    "    print(\"üîÆ Creating embeddings and storing in ChromaDB...\")\n",
    "        \n",
    "    embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "    \n",
    "    # Create ChromaDB vector store\n",
    "    print(\"--- Creating vector store ---\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory, \n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(\"--- Finished creating vector store ---\")\n",
    "    \n",
    "    print(f\"‚úÖ Vector store created and saved to {persist_directory}\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_ingestion_pipeline(dir_path: str):\n",
    "    \"\"\"Run the complete RAG ingestion pipeline\"\"\"\n",
    "    print(\"üöÄ Starting RAG Ingestion Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Partition\n",
    "    elements = partition_documents(dir_path)\n",
    "    \n",
    "    # Step 2: Chunk\n",
    "    chunks = create_chunks_by_title(elements)\n",
    "    \n",
    "    # Step 3: AI Summarisation\n",
    "    summarised_chunks = summarise_chunks(chunks)\n",
    "    \n",
    "    # Step 4: Vector Store\n",
    "    db = create_vector_store(summarised_chunks, persist_directory=\"dbv2/chroma_db\")\n",
    "    \n",
    "    print(\"üéâ Pipeline completed successfully!\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = run_complete_ingestion_pipeline(\"./pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the main result of the paper of ganguly from which soundarajan's paper on smooth numbers in short intervals is inspired?\"\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "\n",
    "def generate_final_answer(chunks, query):\n",
    "    \"\"\"Generate final answer using multimodal content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-2.5-flash\",\n",
    "                    temperature=0.0,  # Gemini 3.0+ defaults to 1.0\n",
    "                    max_tokens=None,\n",
    "                    timeout=120000,\n",
    "                    max_retries=3\n",
    "                )\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"Based on the following documents, please answer this question: {query}\n",
    "\n",
    "CONTENT TO ANALYZE:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prompt_text += f\"--- Document {i+1} ---\\n\"\n",
    "            \n",
    "            if \"original_content\" in chunk.metadata:\n",
    "                original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                \n",
    "                # Add raw text\n",
    "                raw_text = original_data.get(\"raw_text\", \"\")\n",
    "                if raw_text:\n",
    "                    prompt_text += f\"TEXT:\\n{raw_text}\\n\\n\"\n",
    "                \n",
    "                # Add tables as HTML\n",
    "                tables_html = original_data.get(\"tables_html\", [])\n",
    "                if tables_html:\n",
    "                    prompt_text += \"TABLES:\\n\"\n",
    "                    for j, table in enumerate(tables_html):\n",
    "                        prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "            \n",
    "            prompt_text += \"\\n\"\n",
    "        \n",
    "        prompt_text += \"\"\"\n",
    "Please provide a clear, comprehensive answer using the text, tables, and images above. If the documents don't contain sufficient information to answer the question, say \"I don't have enough information to answer that question based on the provided documents.\"\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add all images from all chunks\n",
    "        for chunk in chunks:\n",
    "            if \"original_content\" in chunk.metadata:\n",
    "                original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                images_base64 = original_data.get(\"images_base64\", [])\n",
    "                \n",
    "                for image_base64 in images_base64:\n",
    "                    message_content.append({\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "                    })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Answer generation failed: {e}\")\n",
    "        return \"Sorry, I encountered an error while generating the answer.\"\n",
    "\n",
    "# Usage\n",
    "final_answer = generate_final_answer(chunks, query)\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-papers-rag-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
