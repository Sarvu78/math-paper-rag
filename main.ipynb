{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d951179",
   "metadata": {},
   "source": [
    "# Math-RAG: Interactive Research Paper Analysis\n",
    "Navigating mathematics research papers often leads to a \"citation rabbit hole.\" When a proof relies on a specific Lemma or Theorem from a cited work, researchers must manually find, download, and search through external papers—a process that breaks cognitive flow.\n",
    "\n",
    "Math-RAG is an intelligent Retrieval-Augmented Generation solution designed to make the citation network interactive. By indexing both the primary paper and its references, it provides immediate access to the technical details you need, exactly when you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487b0aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarvagyajain/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Standard Library Imports\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "# ==========================================\n",
    "# 2. Environment & Network Imports\n",
    "# ==========================================\n",
    "import requests\n",
    "from httpx import get\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================\n",
    "# 3. Document Processing (PDFs & Parsing)\n",
    "# ==========================================\n",
    "import fitz  # PyMuPDF for basic PDF manipulation\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# ==========================================\n",
    "# 4. AI & LangChain Framework\n",
    "# ==========================================\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Vector Store & Embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# ==========================================\n",
    "# 5. Specialized Research & Academic Tools\n",
    "# ==========================================\n",
    "import arxiv\n",
    "from rapidfuzz import fuzz\n",
    "from unpywall.utils import UnpywallCredentials\n",
    "from unpywall import Unpywall\n",
    "\n",
    "# ==========================================\n",
    "# 6. Configuration & Initialization\n",
    "# ==========================================\n",
    "\n",
    "# Load environment variables (API Keys, etc.)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Unpywall for open-access paper retrieval\n",
    "UnpywallCredentials('sarvagya07jain@gmail.com')\n",
    "\n",
    "# Constants\n",
    "CROSSREF_API = \"https://api.crossref.org\"\n",
    "\n",
    "# Directory Setup\n",
    "# Note: Using Pathlib is generally safer for cross-platform paths\n",
    "PROJECT_DIR = Path(\"/Users/sarvagyajain/Downloads/Programming/math-papers-rag\")\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9712f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Opens a PDF and locates the start of the bibliography section.\n",
    "    Returns all text following the 'References' or 'Bibliography' header.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    # Extract text from all pages and join into a single string\n",
    "    text = \"\".join(p.get_text() for p in doc)\n",
    "\n",
    "    # Search for the headers commonly used in academic papers\n",
    "    match = re.search(\n",
    "        r\"\\nreferences\\n|\\nbibliography\\n\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    if not match:\n",
    "        raise ValueError(\"References section not found - check if the PDF uses a different header.\")\n",
    "\n",
    "    # Return only the text starting from the end of the matched header\n",
    "    return text[match.end():]\n",
    "\n",
    "def split_references(ref_text):\n",
    "    \"\"\"\n",
    "    Attempts to split a block of reference text into individual citations \n",
    "    using common academic numbering and labeling patterns.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"\\n\\s*\\[\\d+\\]\\s*\",       # Matches [1], [2], etc.\n",
    "        r\"\\n\\s*\\d+\\.\\s*\",         # Matches 1., 2., etc.\n",
    "        r\"\\n\\s*\\[[A-Z]{2,}\\d{2}\\]\\s*\" # Matches alphanumeric labels like [SJ24]\n",
    "    ]\n",
    "\n",
    "    for p in patterns:\n",
    "        parts = re.split(p, ref_text)\n",
    "        # Heuristic: If we found more than 3 parts, the pattern likely worked.\n",
    "        # We also filter for length (>50 chars) to ignore noise or page numbers.\n",
    "        if len(parts) > 3:\n",
    "            return [r.strip() for r in parts if len(r.strip()) > 50]\n",
    "\n",
    "    # Fallback: If no pattern fits, split by newline and filter for longer strings.\n",
    "    return [r.strip() for r in ref_text.split(\"\\n\") if len(r.strip()) > 80]\n",
    "\n",
    "def normalize_reference_text(ref):\n",
    "    \"\"\"\n",
    "    Cleans up a raw reference string by removing excessive whitespace, \n",
    "    new lines, and potential page artifacts.\n",
    "    \"\"\"\n",
    "    # Replace all whitespace (tabs, multiple spaces, newlines) with a single space\n",
    "    ref = re.sub(r\"\\s+\", \" \", ref)\n",
    "    ref = ref.replace(\"\\n\", \" \")\n",
    "    ref = ref.strip(\" .;,\")\n",
    "\n",
    "    # Filter out potential artifacts (e.g., page headers or footers) \n",
    "    # that are too short to be a valid citation.\n",
    "    if len(ref.split()) < 6:\n",
    "        return None\n",
    "\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d52bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_title_extraction(reference):\n",
    "    if not reference:\n",
    "        return None\n",
    "\n",
    "    # 1. PRE-PROCESSING: Clean up the raw string\n",
    "    # Replace newlines with spaces and handle hyphenated line breaks (mag-nitude -> magnitude)\n",
    "    ref = reference.replace('\\n', ' ')\n",
    "    ref = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', ref) \n",
    "    \n",
    "    # Remove leading citation numbers like [12] or 12.\n",
    "    ref = re.sub(r'^\\[?\\d+\\]?\\s*', '', ref)\n",
    "\n",
    "    # 2. STRATEGY: Split into candidates\n",
    "    # We split by common delimiters: Periods, Commas, and Semicolons\n",
    "    candidates = re.split(r'[.,;]\\s+', ref)\n",
    "    \n",
    "    best_candidate = None\n",
    "    highest_score = -100\n",
    "\n",
    "    # Markers that suggest a segment is NOT a title\n",
    "    forbidden_markers = {\n",
    "        'proc', 'vol', 'no', 'pp', 'pages', 'journal', 'theory', 'press', \n",
    "        'university', 'arxiv', 'preprint', 'appear', 'edited', 'eds', 'acta', \n",
    "        'annals', 'math', 'soc', 'inst', 'conf', 'berkeley', 'cambridge', 'ann'\n",
    "    }\n",
    "    \n",
    "    # Math markers that increase confidence\n",
    "    math_markers = {\n",
    "        'integers', 'prime', 'factors', 'short', 'intervals', 'smooth', \n",
    "        'analytic', 'riemann', 'hypothesis', 'elliptic', 'curves', 'asymptotic',\n",
    "        'expansion', 'distribution', 'function'\n",
    "    }\n",
    "\n",
    "    for segment in candidates:\n",
    "        segment = segment.strip().strip(\".,()[]\")\n",
    "        words = segment.split()\n",
    "        word_count = len(words)\n",
    "        lower_seg = segment.lower()\n",
    "\n",
    "        if word_count < 3:\n",
    "            continue\n",
    "            \n",
    "        score = 0\n",
    "\n",
    "        # --- Scoring Logic ---\n",
    "        \n",
    "        # Length: Math titles in your list are usually 5-15 words\n",
    "        if 5 <= word_count <= 18:\n",
    "            score += 25\n",
    "        elif 3 <= word_count < 5:\n",
    "            score += 5\n",
    "\n",
    "        # Content: Reward math-heavy terminology\n",
    "        matches = sum(1 for m in math_markers if m in lower_seg)\n",
    "        score += (matches * 8)\n",
    "\n",
    "        # Content: Penalize metadata words (Journal names, \"to appear\", etc.)\n",
    "        # If the segment IS the journal (e.g., \"Int. J. Number Theory\"), penalize heavily\n",
    "        if any(marker in lower_seg for marker in forbidden_markers):\n",
    "            score -= 40\n",
    "            \n",
    "        # Penalize segments that look like author lists (e.g., \"A. Balog\" or \"H. W. Lenstra, Jr.\")\n",
    "        if re.search(r'\\b[A-Z]\\.\\s', segment) or lower_seg.endswith(' jr'):\n",
    "            score -= 30\n",
    "\n",
    "        # Heuristic: Titles often start with \"On \", \"A \", \"The \", \"An \"\n",
    "        if re.match(r'^(on|a|the|an)\\s', lower_seg):\n",
    "            score += 15\n",
    "\n",
    "        if score > highest_score:\n",
    "            highest_score = score\n",
    "            best_candidate = segment\n",
    "\n",
    "    # Final polish: strip trailing punctuation often left by the split\n",
    "    return best_candidate.strip() if best_candidate and highest_score > 0 else None\n",
    "\n",
    "def title_cleanup_manual(raw_title):\n",
    "    \"\"\"\n",
    "    Sanitizes a title string by removing XML/MathML tags, \n",
    "    illegal file characters, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not raw_title:\n",
    "        return None\n",
    "\n",
    "    # 1. Remove MathML tags (e.g., <mml:math>...) frequently found in \n",
    "    # metadata from Crossref or PubMed.\n",
    "    clean_title = re.sub(r'<mml:math[^>]*>.*?<\\/mml:math>', '', raw_title, flags=re.DOTALL)\n",
    "    \n",
    "    # 2. Standardize spacing: Replace newlines with a single space\n",
    "    clean_title = clean_title.replace('\\n', ' ')\n",
    "    \n",
    "    # 3. File System Safety: Remove characters that are illegal in filenames \n",
    "    # (useful if you plan to save the PDF using this title).\n",
    "    clean_title = re.sub(r'[\\\\/:*?\"<>|]', '', clean_title)\n",
    "    \n",
    "    # 4. Final Polish: Collapse multiple spaces into one and lowercase\n",
    "    clean_title = re.sub(r'\\s+', ' ', clean_title).strip()\n",
    "    \n",
    "    return clean_title.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55634411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional block of code for title cleanup using llm.\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-3-pro-preview\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=None,\n",
    ")\n",
    "class CleanReferences(BaseModel):\n",
    "    references: List[str]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are cleaning mathematical bibliography entries.\\n\"\n",
    "     \"You MUST NOT invent references.\\n\"\n",
    "     \"You MUST preserve all factual content.\\n\"\n",
    "     \"You may only normalize formatting, punctuation, and spacing.\\n\"\n",
    "     \"If an entry is not a real bibliographic reference, REMOVE it.\\n\"\n",
    "     \"Return references in MathSciNet-like style.\"\n",
    "    ),\n",
    "    (\"user\",\n",
    "     \"Here are raw references extracted from a math paper:\\n\\n\"\n",
    "     \"{refs}\\n\\n\"\n",
    "     \"Return a JSON object with key 'references'.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "def llm_clean_references_langchain(raw_refs, llm):\n",
    "    parser = JsonOutputParser(pydantic_object=CleanReferences)\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    result = chain.invoke({\n",
    "        \"refs\": \"\\n\".join(f\"- {r}\" for r in raw_refs)\n",
    "    })\n",
    "\n",
    "    return result[\"references\"]\n",
    "\n",
    "\n",
    "class ReferenceMetadata(BaseModel):\n",
    "    title: Optional[str]\n",
    "    authors: Optional[List[str]]\n",
    "\n",
    "metadata_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You extract bibliographic metadata from mathematical references.\\n\"\n",
    "     \"You MUST NOT invent missing information.\\n\"\n",
    "     \"If information is unclear, return null.\\n\"\n",
    "     \"Do NOT guess titles or authors.\\n\"\n",
    "     \"Return structured JSON only.\"\n",
    "    ),\n",
    "    (\"user\",\n",
    "     \"Reference:\\n{ref}\\n\\n\"\n",
    "     \"Extract metadata using the provided schema.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "def llm_extract_reference_metadata(ref, llm):\n",
    "    parser = JsonOutputParser(pydantic_object=ReferenceMetadata)\n",
    "    chain = metadata_prompt | llm | parser\n",
    "\n",
    "    return chain.invoke({\"ref\": ref})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c88e1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv_pdf(title, path=\"./pdfs\"):\n",
    "    \"\"\"\n",
    "    Searches arXiv for a paper by title, validates the match using fuzzy string \n",
    "    comparison, and downloads the PDF if it meets a 90% similarity threshold.\n",
    "    \"\"\"\n",
    "    # 1. Setup download directory\n",
    "    full_path = PROJECT_DIR / path\n",
    "    full_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 2. Initialize arXiv client and search\n",
    "    client = arxiv.Client()\n",
    "    # We search for the top 5 results to account for similar titles\n",
    "    search = arxiv.Search(query=f\"{title}\", max_results=5)\n",
    "\n",
    "    best_score = 0\n",
    "    best_pdf = None\n",
    "\n",
    "    # 3. Identify the most relevant result\n",
    "    for result in client.results(search):\n",
    "        # fuzzy logic handles slight punctuation or casing differences\n",
    "        score = fuzz.ratio(title.lower(), result.title.lower())\n",
    "        \n",
    "        # We only consider high-confidence matches (>90%)\n",
    "        if score > best_score and score > 90:\n",
    "            best_score = score\n",
    "            best_pdf = result\n",
    "            \n",
    "    # 4. Download if a confident match was found\n",
    "    if best_pdf:\n",
    "        # Generate a clean filename using your title_cleanup helper\n",
    "        safe_filename = f\"{title_cleanup_manual(best_pdf.title)}.pdf\"\n",
    "        download_dest = full_path / safe_filename\n",
    "        \n",
    "        # Use the arxiv library's built-in download method\n",
    "        best_pdf.download_pdf(dirpath=str(full_path), filename=safe_filename)\n",
    "        print(f\"Successfully downloaded: {safe_filename} (Score: {best_score})\")\n",
    "    else:\n",
    "        print(f\"No high-confidence match found for: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68eac7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, filename, path=\"./pdfs\"):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from a direct URL (useful for Crossref/Unpywall links).\n",
    "    \"\"\"\n",
    "    # 1. Resolve target directory\n",
    "    full_path = PROJECT_DIR / path\n",
    "    full_path.mkdir(parents=True, exist_ok=True)\n",
    "    target_file = full_path / filename\n",
    "\n",
    "    try:\n",
    "        # 2. Fetch the content with a 20-second timeout\n",
    "        r = requests.get(url, timeout=20)\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            print(f\"Failed to download. Status code: {r.status_code}\")\n",
    "            return None\n",
    "\n",
    "        # 3. Write binary content to file\n",
    "        with open(target_file, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "        return str(target_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during download: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c81b1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doi_by_title(title, rows=3):\n",
    "    \"\"\"\n",
    "    Queries the Crossref API to find the most likely DOI for a given paper title.\n",
    "    \n",
    "    Returns the DOI of the top result based on Crossref's internal \n",
    "    relevance ranking.\n",
    "    \"\"\"\n",
    "    # Crossref API parameters: \n",
    "    # query.title searches the title field specifically\n",
    "    # rows limits the result set (we usually only need the top match)\n",
    "    params = {\n",
    "        \"query.title\": title,\n",
    "        \"rows\": rows\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(f\"{CROSSREF_API}/works\", params=params, timeout=10)\n",
    "        r.raise_for_status() # Raise an error for 4xx or 5xx responses\n",
    "\n",
    "        # Navigate the JSON response structure\n",
    "        items = r.json()[\"message\"][\"items\"]\n",
    "\n",
    "        if not items:\n",
    "            return None\n",
    "\n",
    "        # Crossref sorts by relevance; we assume the first item is the best match\n",
    "        return items[0].get(\"DOI\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding DOI for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_title_from_doi(doi):\n",
    "    \"\"\"\n",
    "    Performs a reverse lookup: takes a DOI and retrieves the official \n",
    "    title of the paper from Crossref.\n",
    "    \"\"\"\n",
    "    # Direct endpoint for a specific work\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        data = r.json()[\"message\"]\n",
    "        \n",
    "        # 'title' is returned as a list of strings in Crossref metadata\n",
    "        titles = data.get(\"title\", [])\n",
    "\n",
    "        # Return the primary title if it exists\n",
    "        return titles[0] if titles else None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving title for DOI {doi}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71730a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cited_dois(doi):\n",
    "    \"\"\"\n",
    "    Given a primary paper's DOI, fetches its reference list and attempts \n",
    "    to download the full-text PDF for every cited work found.\n",
    "    \"\"\"\n",
    "    # 1. Fetch metadata for the parent paper from Crossref\n",
    "    r = requests.get(\n",
    "        f\"{CROSSREF_API}/works/{doi}\",\n",
    "        timeout=10\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "\n",
    "    message = r.json()[\"message\"]\n",
    "    # Extract the list of cited references\n",
    "    references = message.get(\"reference\", [])\n",
    "\n",
    "    for ref in references:\n",
    "        ref_title = ref.get(\"volume-title\")\n",
    "        ref_doi = ref.get(\"DOI\")\n",
    "\n",
    "        # CASE A: The citation has a DOI (Most reliable)\n",
    "        if ref_doi:\n",
    "            # Try to find a legal Open Access PDF link via Unpywall\n",
    "            url = Unpywall.get_pdf_link(doi=ref_doi)\n",
    "            \n",
    "            # Fetch the official title to use as a filename\n",
    "            raw_title = get_title_from_doi(ref_doi)\n",
    "\n",
    "            if raw_title:\n",
    "                title = title_cleanup_manual(raw_title)\n",
    "            else:\n",
    "                title = \"untitled\" # Fallback for metadata-poor records\n",
    "\n",
    "            if url:\n",
    "                filename = f\"{title}.pdf\"\n",
    "                \n",
    "                # Sanity check: if cleaning the title resulted in an empty string\n",
    "                if not title or title == \"untitled\":\n",
    "                    # Use a sanitized DOI as the filename to prevent overwrites\n",
    "                    filename = f\"{ref_doi.replace('/', '_')}.pdf\"\n",
    "                \n",
    "                print(f\"Downloading cited paper via Unpywall: {filename}\")\n",
    "                download_pdf(url, filename)\n",
    "        \n",
    "        # CASE B: No DOI found, but we have a title (Fallback to arXiv)\n",
    "        elif ref_title:\n",
    "            print(f\"No DOI for citation. Searching arXiv for: {ref_title}\")\n",
    "            search_arxiv_pdf(ref_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9aa877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_reference_download(title=None, pdf_path=None):\n",
    "    \"\"\"\n",
    "    The main pipeline for gathering research materials. \n",
    "    Can either:\n",
    "    1. Parse a LOCAL PDF to find and download its references.\n",
    "    2. Use a PAPER TITLE to find its DOI, download itself, and crawl its citations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # PATH 1: Processing a Local PDF File\n",
    "    if pdf_path:\n",
    "        # Extract the bibliography section text\n",
    "        ref_text = extract_references_from_pdf(pdf_path)\n",
    "        # Identify individual citations\n",
    "        raw_references = split_references(ref_text)\n",
    "        # Clean and filter out noise/short strings\n",
    "        normalized_references = [normalize_reference_text(r) for r in raw_references]\n",
    "        final_references = [r for r in normalized_references if r]\n",
    "        \n",
    "        # Pull the title from each citation string\n",
    "        titles = [manual_title_extraction(r) for r in final_references if manual_title_extraction(r)]\n",
    "        \n",
    "        for raw_title in titles:\n",
    "            try:\n",
    "                # Direct search on arXiv for these titles\n",
    "                search_arxiv_pdf(raw_title)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading reference from arxiv: {raw_title}, Error: {e}\")\n",
    "\n",
    "    # PATH 2: Discovery via Paper Title\n",
    "    if title:\n",
    "        # Step 2a: Locate the unique identifier (DOI) for the paper\n",
    "        try:\n",
    "            ref_doi = find_doi_by_title(title)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding DOI for title: {title}, Error: {e}\")\n",
    "            return # Exit if we can't identify the primary paper\n",
    "\n",
    "        # Step 2b: Download the primary paper itself\n",
    "        try:\n",
    "            url = Unpywall.get_pdf_link(doi=ref_doi)\n",
    "            raw_title = get_title_from_doi(ref_doi)\n",
    "\n",
    "            # Cleanup name for file system compatibility\n",
    "            if raw_title:\n",
    "                title = title_cleanup_manual(raw_title)\n",
    "            else:\n",
    "                title = \"untitled\"\n",
    "            \n",
    "            if url:\n",
    "                filename = f\"{title}.pdf\"\n",
    "                if not title:\n",
    "                    filename = f\"{ref_doi.replace('/', '_')}.pdf\"\n",
    "                download_pdf(url, filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading main paper PDF: {title}, DOI: {ref_doi}, Error: {e}\")\n",
    "        try:\n",
    "            # If we couldn't download via Unpywall, try arXiv as a fallback\n",
    "            if not url:\n",
    "                print(f\"No Unpywall link found for DOI: {ref_doi}. Attempting arXiv search.\")\n",
    "                search_arxiv_pdf(title)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading main paper PDF from arXiv: {title}, Error: {e}\")\n",
    "\n",
    "        # Step 2c: Recursively download all citations listed in the metadata\n",
    "        try:\n",
    "            download_cited_dois(ref_doi)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading all cited DOIs for DOI: {ref_doi}, Error: {e}\")\n",
    "    \n",
    "    print(\"✅ Completed full reference download pipeline\")\n",
    "\n",
    "    # Safety check for missing arguments\n",
    "    if not title and not pdf_path:\n",
    "        raise ValueError(\"Must provide either a 'title' or a 'pdf_path' to initiate the pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b68455c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No high-confidence match found for: A\n",
      "No high-confidence match found for: Croot, Smooth numbers in short intervals, Int\n",
      "No high-confidence match found for: Dickman, On the frequency of numbers containing prime factors of a certain relative magnitude, Ark\n",
      "No high-confidence match found for: S\n",
      "No high-confidence match found for: A\n",
      "No high-confidence match found for: J\n",
      "No high-confidence match found for: Harman, Short intervals containing numbers without large prime factors, Math\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfull_reference_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msmooth numbers in short intervals\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./pdfs/smooth numbers in short intervals.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mfull_reference_download\u001b[39m\u001b[34m(title, pdf_path)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m raw_title \u001b[38;5;129;01min\u001b[39;00m titles:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     24\u001b[39m         \u001b[38;5;66;03m# Direct search on arXiv for these titles\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         \u001b[43msearch_arxiv_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError downloading reference from arxiv: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_title\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36msearch_arxiv_pdf\u001b[39m\u001b[34m(title, path)\u001b[39m\n\u001b[32m     16\u001b[39m best_pdf = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 3. Identify the most relevant result\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# fuzzy logic handles slight punctuation or casing differences\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuzz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# We only consider high-confidence matches (>90%)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/arxiv/__init__.py:648\u001b[39m, in \u001b[36mClient._results\u001b[39m\u001b[34m(self, search, offset)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, search: Search, offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m) -> Generator[Result, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m    647\u001b[39m     page_url = \u001b[38;5;28mself\u001b[39m._format_url(search, offset, \u001b[38;5;28mself\u001b[39m.page_size)\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     feed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feed.entries:\n\u001b[32m    650\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mGot empty first page; stopping generation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/arxiv/__init__.py:695\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    689\u001b[39m \u001b[33;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[32m    690\u001b[39m \n\u001b[32m    691\u001b[39m \u001b[33;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m`self.num_retries` times.\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    697\u001b[39m     HTTPError,\n\u001b[32m    698\u001b[39m     UnexpectedEmptyPageError,\n\u001b[32m    699\u001b[39m     requests.exceptions.ConnectionError,\n\u001b[32m    700\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    701\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _try_index < \u001b[38;5;28mself\u001b[39m.num_retries:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/arxiv/__init__.py:729\u001b[39m, in \u001b[36mClient.__try_parse_feed\u001b[39m\u001b[34m(self, url, first_page, try_index)\u001b[39m\n\u001b[32m    725\u001b[39m         time.sleep(to_sleep)\n\u001b[32m    727\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, first_page, try_index, url)\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser-agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marxiv.py/2.3.2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[38;5;28mself\u001b[39m._last_request_dt = datetime.now()\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status_code != requests.codes.OK:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Programming/math-papers-rag/.venv/lib/python3.13/site-packages/urllib3/connection.py:571\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    568\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    570\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    574\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.11/lib/python3.13/http/client.py:1450\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1448\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1450\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1452\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.11/lib/python3.13/http/client.py:336\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    338\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.11/lib/python3.13/http/client.py:297\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    299\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.11/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.11/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.11/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "full_reference_download(title = \"smooth numbers in short intervals\", pdf_path=\"./pdfs/smooth numbers in short intervals.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "619815f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your folder path\n",
    "def partition_documents(input_dir: str):\n",
    "    input_dir = Path(input_dir)\n",
    "    all_elements = []\n",
    "\n",
    "    # 2. Loop through every PDF in the directory\n",
    "    for pdf_file in input_dir.glob(\"*.pdf\"):\n",
    "        print(f\"Processing: {pdf_file.name}...\")\n",
    "        \n",
    "        # 3. Partition the PDF into elements (Titles, NarrativeText, Tables, etc.)\n",
    "        elements = partition_pdf(\n",
    "            filename=str(pdf_file),\n",
    "            # Strategy 'hi_res' is best for RAG as it identifies tables/images\n",
    "            strategy=\"hi_res\", \n",
    "            # Optional: merge small chunks to maintain context\n",
    "            combine_text_under_n_chars=500,\n",
    "            # Extract images or tables if needed for multimodal RAG\n",
    "            extract_images_in_pdf=False \n",
    "        )\n",
    "        \n",
    "        all_elements.extend(elements)\n",
    "\n",
    "\n",
    "    print(f\"Total elements captured: {len(all_elements)}\")\n",
    "    return all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "541e5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_by_title(elements):\n",
    "    \"\"\"Create intelligent chunks using title-based strategy\"\"\"\n",
    "    print(\"🔨 Creating smart chunks...\")\n",
    "    \n",
    "    chunks = chunk_by_title(\n",
    "        elements, # The parsed PDF elements from previous step\n",
    "        max_characters=3000, # Hard limit - never exceed 3000 characters per chunk\n",
    "        new_after_n_chars=2400, # Try to start a new chunk after 2400 characters\n",
    "        combine_text_under_n_chars=500 # Merge tiny chunks under 500 chars with neighbors\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Created {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c682584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_content_types(chunk):\n",
    "    \"\"\"Analyze what types of content are in a chunk\"\"\"\n",
    "    content_data = {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'types': ['text']\n",
    "    }\n",
    "    \n",
    "    # Check for tables and images in original elements\n",
    "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            element_type = type(element).__name__\n",
    "            \n",
    "            # Handle tables\n",
    "            if element_type == 'Table':\n",
    "                content_data['types'].append('table')\n",
    "                table_html = getattr(element.metadata, 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "            \n",
    "            # Handle images\n",
    "            elif element_type == 'Image':\n",
    "                if hasattr(element, 'metadata') and hasattr(element.metadata, 'image_base64'):\n",
    "                    content_data['types'].append('image')\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "    \n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d499be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ai_enhanced_summary(text: str, tables: List[str], images: List[str]) -> str:\n",
    "    \"\"\"Create AI-enhanced summary for mixed content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-2.5-flash\",\n",
    "                    temperature=0.0,  # Gemini 3.0+ defaults to 1.0\n",
    "                    max_tokens=None,\n",
    "                    timeout=120000,\n",
    "                    max_retries=10\n",
    "                )\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "\n",
    "        CONTENT TO ANALYZE:\n",
    "        TEXT CONTENT:\n",
    "        {text}\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add tables if present\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i, table in enumerate(tables):\n",
    "                prompt_text += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "        \n",
    "                prompt_text += \"\"\"\n",
    "                YOUR TASK:\n",
    "                Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "                1. Key facts, numbers, and data points from text and tables\n",
    "                2. Main topics and concepts discussed  \n",
    "                3. Questions this content could answer\n",
    "                4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "                5. Alternative search terms users might use\n",
    "\n",
    "                Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "                SEARCHABLE DESCRIPTION:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add images to the message\n",
    "        for image_base64 in images:\n",
    "            message_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "            })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ❌ AI summary failed: {e}\")\n",
    "        # Fallback to simple summary\n",
    "        summary = f\"{text[:300]}...\"\n",
    "        if tables:\n",
    "            summary += f\" [Contains {len(tables)} table(s)]\"\n",
    "        if images:\n",
    "            summary += f\" [Contains {len(images)} image(s)]\"\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d5314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_chunks(chunks):\n",
    "    \"\"\"Process all chunks with AI Summaries\"\"\"\n",
    "    print(\"🧠 Processing chunks with AI Summaries...\")\n",
    "    \n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk = i + 1\n",
    "        print(f\"   Processing chunk {current_chunk}/{total_chunks}\")\n",
    "        \n",
    "        # Analyze chunk content\n",
    "        content_data = separate_content_types(chunk)\n",
    "        \n",
    "        # Debug prints\n",
    "        print(f\"     Types found: {content_data['types']}\")\n",
    "        print(f\"     Tables: {len(content_data['tables'])}, Images: {len(content_data['images'])}\")\n",
    "        \n",
    "        # Create AI-enhanced summary if chunk has tables/images\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(f\"     → Creating AI summary for mixed content...\")\n",
    "            try:\n",
    "                enhanced_content = create_ai_enhanced_summary(\n",
    "                    content_data['text'],\n",
    "                    content_data['tables'], \n",
    "                    content_data['images']\n",
    "                )\n",
    "                print(f\"     → AI summary created successfully\")\n",
    "                print(f\"     → Enhanced content preview: {enhanced_content[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ❌ AI summary failed: {e}\")\n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(f\"     → Using raw text (no tables/images)\")\n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        # Create LangChain Document with rich metadata\n",
    "        doc = Document(\n",
    "            page_content=enhanced_content,\n",
    "            metadata={\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": content_data['text'],\n",
    "                    \"tables_html\": content_data['tables'],\n",
    "                    \"images_base64\": content_data['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        langchain_documents.append(doc)\n",
    "    \n",
    "    print(f\"✅ Processed {len(langchain_documents)} chunks\")\n",
    "    return langchain_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "296d0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_chunks_to_json(chunks, filename=\"chunks_export.json\"):\n",
    "    \"\"\"Export processed chunks to clean JSON format\"\"\"\n",
    "    export_data = []\n",
    "    \n",
    "    for i, doc in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"enhanced_content\": doc.page_content,\n",
    "            \"metadata\": {\n",
    "                \"original_content\": json.loads(doc.metadata.get(\"original_content\", \"{}\"))\n",
    "            }\n",
    "        }\n",
    "        export_data.append(chunk_data)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Exported {len(export_data)} chunks to {filename}\")\n",
    "    return export_data\n",
    "\n",
    "# Export your chunks\n",
    "# json_data = export_chunks_to_json(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "031a3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents, persist_directory=\"dbv1/chroma_db\"):\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    \"\"\"Create and persist ChromaDB vector store\"\"\"\n",
    "    print(\"🔮 Creating embeddings and storing in ChromaDB...\")\n",
    "        \n",
    "    embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "    \n",
    "    # Create ChromaDB vector store\n",
    "    print(\"--- Creating vector store ---\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory, \n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(\"--- Finished creating vector store ---\")\n",
    "    \n",
    "    print(f\"✅ Vector store created and saved to {persist_directory}\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8129e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_ingestion_pipeline(dir_path: str):\n",
    "    \"\"\"Run the complete RAG ingestion pipeline\"\"\"\n",
    "    print(\"🚀 Starting RAG Ingestion Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Partition\n",
    "    elements = partition_documents(dir_path)\n",
    "    \n",
    "    # Step 2: Chunk\n",
    "    chunks = create_chunks_by_title(elements)\n",
    "    \n",
    "    # Step 3: AI Summarisation\n",
    "    summarised_chunks = summarise_chunks(chunks)\n",
    "    \n",
    "    # Step 4: Vector Store\n",
    "    db = create_vector_store(summarised_chunks, persist_directory=\"dbv2/chroma_db\")\n",
    "    \n",
    "    print(\"🎉 Pipeline completed successfully!\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e131f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting RAG Ingestion Pipeline\n",
      "==================================================\n",
      "Processing: smooth numbers in short intervals.pdf...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Processing: on smooth integers in short intervals under the riemann hypothesis.pdf...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Processing: integers without large prime factors in short intervals and arithmetic progressions.pdf...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Total elements captured: 558\n",
      "🔨 Creating smart chunks...\n",
      "✅ Created 17 chunks\n",
      "🧠 Processing chunks with AI Summaries...\n",
      "   Processing chunk 1/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 2/17\n",
      "     Types found: ['text', 'image']\n",
      "     Tables: 0, Images: 1\n",
      "     → Creating AI summary for mixed content...\n",
      "     ❌ AI summary failed: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 22.010675847s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '22s'}]}}\n",
      "     → AI summary created successfully\n",
      "     → Enhanced content preview: K. SOUNDARARAJAN\n",
      "\n",
      "One may wonder if the Riemann Hypothesis is of use in this problem. Assuming RH, Xuan [15] has shown that intervals [x,x + x 1 2(logx)1+ǫ] contain xǫ-smooth integers. Recently Gangul...\n",
      "   Processing chunk 3/17\n",
      "     Types found: ['text', 'image']\n",
      "     Tables: 0, Images: 5\n",
      "     → Creating AI summary for mixed content...\n",
      "     ❌ AI summary failed: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 14.230337201s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '14s'}]}}\n",
      "     → AI summary created successfully\n",
      "     → Enhanced content preview: Hence\n",
      "\n",
      "25 n I= log — tog @ n x > a<n<aers » my mg eS) Vey 1/3 <my,mos Vey 1/4 A(r) min (\n",
      "\n",
      "m1,m2≤\n",
      "\n",
      "≤\n",
      "\n",
      "Note that r = n/(m1m2) is at most y, and so the integers n counted in the RHS above are all y-smoot...\n",
      "   Processing chunk 4/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 5/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 6/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 7/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 8/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 9/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 10/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 11/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 12/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 13/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 14/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 15/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "   Processing chunk 16/17\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "     → Creating AI summary for mixed content...\n",
      "     ❌ AI summary failed: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 4.368890259s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '4s'}]}}\n",
      "     → AI summary created successfully\n",
      "     → Enhanced content preview: while the cube-free case requires q1/3 to be replaced by q1/4 in the above.\n",
      "\n",
      "Integers without large prime factors\n",
      "\n",
      "References\n",
      "\n",
      "[1] R. C. Baker, The greatest prime factor of the integers in an interval...\n",
      "   Processing chunk 17/17\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "     → Using raw text (no tables/images)\n",
      "✅ Processed 17 chunks\n",
      "🔮 Creating embeddings and storing in ChromaDB...\n",
      "--- Creating vector store ---\n",
      "--- Finished creating vector store ---\n",
      "✅ Vector store created and saved to dbv2/chroma_db\n",
      "🎉 Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "db = run_complete_ingestion_pipeline(\"./pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c2949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Answer generation failed: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 23.018411731s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '23s'}]}}\n",
      "Sorry, I encountered an error while generating the answer.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the main result of the paper of ganguly from which soundarajan's paper on smooth numbers in short intervals is inspired?\"\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "\n",
    "def generate_final_answer(chunks, query):\n",
    "    \"\"\"Generate final answer using multimodal content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-2.5-flash\",\n",
    "                    temperature=0.0,  # Gemini 3.0+ defaults to 1.0\n",
    "                    max_tokens=None,\n",
    "                    timeout=120000,\n",
    "                    max_retries=3\n",
    "                )\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"Based on the following documents, please answer this question: {query}\n",
    "\n",
    "CONTENT TO ANALYZE:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prompt_text += f\"--- Document {i+1} ---\\n\"\n",
    "            \n",
    "            if \"original_content\" in chunk.metadata:\n",
    "                original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                \n",
    "                # Add raw text\n",
    "                raw_text = original_data.get(\"raw_text\", \"\")\n",
    "                if raw_text:\n",
    "                    prompt_text += f\"TEXT:\\n{raw_text}\\n\\n\"\n",
    "                \n",
    "                # Add tables as HTML\n",
    "                tables_html = original_data.get(\"tables_html\", [])\n",
    "                if tables_html:\n",
    "                    prompt_text += \"TABLES:\\n\"\n",
    "                    for j, table in enumerate(tables_html):\n",
    "                        prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "            \n",
    "            prompt_text += \"\\n\"\n",
    "        \n",
    "        prompt_text += \"\"\"\n",
    "Please provide a clear, comprehensive answer using the text, tables, and images above. If the documents don't contain sufficient information to answer the question, say \"I don't have enough information to answer that question based on the provided documents.\"\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add all images from all chunks\n",
    "        for chunk in chunks:\n",
    "            if \"original_content\" in chunk.metadata:\n",
    "                original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                images_base64 = original_data.get(\"images_base64\", [])\n",
    "                \n",
    "                for image_base64 in images_base64:\n",
    "                    message_content.append({\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "                    })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Answer generation failed: {e}\")\n",
    "        return \"Sorry, I encountered an error while generating the answer.\"\n",
    "\n",
    "# Usage\n",
    "final_answer = generate_final_answer(chunks, query)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a85f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d597727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-papers-rag-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
